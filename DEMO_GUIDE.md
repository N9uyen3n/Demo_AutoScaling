# üéØ H∆∞·ªõng d·∫´n Ho√†n th√†nh Demo v·ªõi D·ªØ li·ªáu & Model Th·∫≠t

## üìã Checklist t·ªïng quan

- [ ] **B∆∞·ªõc 1**: Chu·∫©n b·ªã d·ªØ li·ªáu th·∫≠t
- [ ] **B∆∞·ªõc 2**: Train model v·ªõi d·ªØ li·ªáu th·∫≠t
- [ ] **B∆∞·ªõc 3**: T√≠ch h·ª£p v√†o dashboard
- [ ] **B∆∞·ªõc 4**: Test v√† verify
- [ ] **B∆∞·ªõc 5**: Chu·∫©n b·ªã presentation

---

## üî• B∆Ø·ªöC 1: Chu·∫©n b·ªã D·ªØ li·ªáu Th·∫≠t

### Option A: N·∫øu b·∫°n ƒë√£ c√≥ d·ªØ li·ªáu

**Format y√™u c·∫ßu**: CSV file v·ªõi columns:
```csv
timestamp,requests_per_minute
2026-01-20 00:00:00,85
2026-01-20 00:01:00,92
2026-01-20 00:02:00,98
...
```

**Validation script**:
```python
# validate_data.py
from utils import DataLoader

# Load data
df = DataLoader.load_from_csv('DATA/your_real_data.csv')

# Validate
validation = DataLoader.validate_data(df)

if validation['is_valid']:
    print("‚úÖ Data is valid!")
    print(f"Total records: {len(df)}")
    print(f"Time range: {df['timestamp'].min()} to {df['timestamp'].max()}")
    print(f"Avg load: {df['requests_per_minute'].mean():.1f} req/m")
else:
    print("‚ùå Data has errors:")
    for error in validation['errors']:
        print(f"  - {error}")
    for warning in validation['warnings']:
        print(f"  ‚ö†Ô∏è {warning}")
```

**Ch·∫°y validation**:
```bash
.venv\Scripts\Activate.ps1
python validate_data.py
```

### Option B: N·∫øu ch∆∞a c√≥ d·ªØ li·ªáu

**Collect t·ª´ h·ªá th·ªëng th·∫≠t**:
1. Monitoring system (Prometheus, CloudWatch, etc.)
2. Application logs
3. Load balancer metrics

**Ho·∫∑c generate realistic data**:
```python
# generate_realistic_data.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def generate_realistic_traffic(days=7):
    """Generate realistic traffic pattern"""
    timestamps = []
    loads = []
    
    start_time = datetime.now() - timedelta(days=days)
    
    for i in range(days * 24 * 60):  # minutes in 'days' days
        current_time = start_time + timedelta(minutes=i)
        timestamps.append(current_time)
        
        # Pattern: Low at night, high during day
        hour = current_time.hour
        day_of_week = current_time.weekday()
        
        # Base load
        if 0 <= hour < 6:  # Night
            base = 50
        elif 6 <= hour < 9:  # Morning ramp-up
            base = 80 + (hour - 6) * 20
        elif 9 <= hour < 17:  # Business hours
            base = 140
        elif 17 <= hour < 22:  # Evening
            base = 100
        else:  # Late night
            base = 60
        
        # Weekend reduction
        if day_of_week >= 5:
            base *= 0.7
        
        # Add noise and occasional spikes
        noise = np.random.normal(0, 10)
        spike = 50 if np.random.random() < 0.02 else 0  # 2% chance of spike
        
        load = max(30, int(base + noise + spike))
        loads.append(load)
    
    df = pd.DataFrame({
        'timestamp': timestamps,
        'requests_per_minute': loads
    })
    
    return df

# Generate and save
df = generate_realistic_traffic(days=7)
df.to_csv('DATA/realistic_data.csv', index=False)
print(f"‚úÖ Generated {len(df)} records")
print(f"Time range: {df['timestamp'].min()} to {df['timestamp'].max()}")
print(f"Avg load: {df['requests_per_minute'].mean():.1f} req/m")
```

**Ch·∫°y**:
```bash
python generate_realistic_data.py
```

---

## ü§ñ B∆Ø·ªöC 2: Train Model v·ªõi D·ªØ li·ªáu Th·∫≠t

### Script train_model.py

T·∫°o file `train_model.py`:

```python
"""
Train XGBoost model for load forecasting
"""
import pandas as pd
import numpy as np
import xgboost as xgb
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
from utils import DataLoader, DataPreprocessor

def train_forecasting_model(data_path, horizons=[1, 5, 15]):
    """
    Train XGBoost models for multi-horizon forecasting
    
    Args:
        data_path: Path to CSV data
        horizons: List of forecast horizons (minutes)
    """
    print("üìä Loading data...")
    df = DataLoader.load_from_csv(data_path)
    
    # Validate
    validation = DataLoader.validate_data(df)
    if not validation['is_valid']:
        print("‚ùå Data validation failed!")
        return
    
    print(f"‚úÖ Loaded {len(df)} records")
    
    # Extract features
    print("üîß Extracting features...")
    df = DataPreprocessor.extract_features(df)
    df = DataPreprocessor.create_lag_features(
        df, 
        'requests_per_minute', 
        lags=[1, 5, 15, 30, 60]
    )
    
    # Prepare features
    feature_cols = [
        'hour', 'day_of_week', 'is_weekend', 'minute',
        'requests_per_minute_lag_1',
        'requests_per_minute_lag_5',
        'requests_per_minute_lag_15',
        'requests_per_minute_lag_30',
        'requests_per_minute_lag_60'
    ]
    
    X = df[feature_cols]
    
    # Train model for each horizon
    models = {}
    
    for horizon in horizons:
        print(f"\nüéØ Training model for {horizon}m forecast...")
        
        # Create target (shift backwards to predict future)
        y = df['requests_per_minute'].shift(-horizon)
        
        # Remove NaN
        mask = ~(X.isna().any(axis=1) | y.isna())
        X_clean = X[mask]
        y_clean = y[mask]
        
        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X_clean, y_clean, test_size=0.2, shuffle=False
        )
        
        # Train XGBoost
        model = xgb.XGBRegressor(
            objective='reg:squarederror',
            max_depth=6,
            learning_rate=0.1,
            n_estimators=100,
            subsample=0.8,
            colsample_bytree=0.8
        )
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )
        
        # Evaluate
        y_pred = model.predict(X_test)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        
        print(f"  MAE: {mae:.2f}")
        print(f"  RMSE: {rmse:.2f}")
        print(f"  MAPE: {mape:.2f}%")
        
        # Save model
        model_path = f'models/xgboost_{horizon}m.joblib'
        joblib.dump(model, model_path)
        print(f"  ‚úÖ Saved to {model_path}")
        
        models[horizon] = {
            'model': model,
            'mae': mae,
            'rmse': rmse,
            'mape': mape
        }
    
    # Save feature columns for later use
    joblib.dump(feature_cols, 'models/feature_columns.joblib')
    
    print("\nüéâ Training complete!")
    print("\nModel Performance Summary:")
    for horizon, metrics in models.items():
        print(f"  {horizon}m: MAE={metrics['mae']:.2f}, RMSE={metrics['rmse']:.2f}, MAPE={metrics['mape']:.2f}%")
    
    return models

if __name__ == "__main__":
    # Train with your data
    models = train_forecasting_model('DATA/realistic_data.csv')
```

### C√†i ƒë·∫∑t dependencies

```bash
.venv\Scripts\Activate.ps1
pip install xgboost scikit-learn
```

### Ch·∫°y training

```bash
python train_model.py
```

**Output mong ƒë·ª£i**:
```
üìä Loading data...
‚úÖ Loaded 10080 records
üîß Extracting features...

üéØ Training model for 1m forecast...
  MAE: 8.45
  RMSE: 12.32
  MAPE: 7.23%
  ‚úÖ Saved to models/xgboost_1m.joblib

üéØ Training model for 5m forecast...
  MAE: 12.67
  RMSE: 18.91
  MAPE: 10.45%
  ‚úÖ Saved to models/xgboost_5m.joblib

üéØ Training model for 15m forecast...
  MAE: 18.23
  RMSE: 25.44
  MAPE: 14.67%
  ‚úÖ Saved to models/xgboost_15m.joblib

üéâ Training complete!
```

---

## üîå B∆Ø·ªöC 3: T√≠ch h·ª£p v√†o Dashboard

### Update app.py ƒë·ªÉ s·ª≠ d·ª•ng real model

T·∫°o file `app_production.py` (ho·∫∑c update `app.py`):

```python
"""
PLANORA - Production Mode with Real Data & Model
"""
import streamlit as st
import pandas as pd
from datetime import datetime
import time

from config.settings import SIMULATION_STEPS, INITIAL_REPLICAS, COST_PER_REPLICA, FIXED_REPLICAS
from utils import (
    DataLoader, ModelManager,
    scaling_logic, calculate_cpu_utilization, calculate_cost_savings
)
from components import (
    render_sidebar, render_production_mode_controls,
    create_kpi_placeholders, render_kpi_cards,
    create_traffic_chart, create_resource_gauge,
    render_scaling_events_tab, render_model_performance_tab, render_security_tab,
    render_data_info, render_model_info
)
from styles.aws_theme import get_aws_css

# Page config
st.set_page_config(
    page_title="PLANORA - Production Dashboard",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown(get_aws_css(), unsafe_allow_html=True)

# Initialize session state
def initialize_session_state():
    if 'history' not in st.session_state:
        st.session_state.history = pd.DataFrame(columns=[
            'Time', 'Actual', 'Forecast_1m', 'Forecast_5m', 'Forecast_15m',
            'Replicas', 'CPU_Util', 'Cost_AI', 'Cost_Fixed', 'Status', 'Reason', 'Anomaly'
        ])
        st.session_state.current_replicas = INITIAL_REPLICAS
        st.session_state.last_scale_time = datetime.now()
        st.session_state.total_cost_ai = 0
        st.session_state.total_cost_fixed = 0
        st.session_state.predictions_history = []
        st.session_state.data_loaded = False
        st.session_state.model_loaded = False

def main():
    initialize_session_state()
    
    # Sidebar
    settings = render_sidebar()
    prod_settings = render_production_mode_controls()
    
    # Main header
    st.title("üöÄ PLANORA: Production Autoscaling Dashboard")
    st.markdown("**AWS-Style Auto Scaling | AI-Powered Load Forecasting**")
    
    # Load data if production mode
    data_df = None
    if prod_settings['mode'] == "Production (Real Data)":
        if prod_settings['data_source'] == "Upload CSV" and prod_settings['uploaded_data']:
            data_df = DataLoader.load_from_uploaded_file(prod_settings['uploaded_data'])
            st.session_state.data_loaded = True
        elif prod_settings['data_source'] == "Load from File" and prod_settings.get('data_path'):
            data_df = DataLoader.load_from_csv(prod_settings['data_path'])
            st.session_state.data_loaded = True
        
        if data_df is not None and not data_df.empty:
            render_data_info(data_df)
    
    # Load model if specified
    model_manager = ModelManager()
    if prod_settings.get('model_path'):
        if model_manager.load_model(prod_settings['model_path']):
            st.session_state.model_loaded = True
            render_model_info(model_manager)
    
    # Rest of dashboard...
    # (KPI cards, charts, tabs - same as before)
    
    st.info("üí° **Tip**: Dashboard ƒëang ch·∫°y v·ªõi production mode. D·ªØ li·ªáu v√† model ƒë√£ ƒë∆∞·ª£c load!")

if __name__ == "__main__":
    main()
```

### Ho·∫∑c ƒë∆°n gi·∫£n h∆°n: Ch·ªâ d√πng UI

**Kh√¥ng c·∫ßn code g√¨ th√™m!** Ch·ªâ c·∫ßn:

1. Ch·∫°y `streamlit run app.py`
2. Trong sidebar:
   - Ch·ªçn "Production (Real Data)"
   - Data Source: "Load from File"
   - Data Path: `DATA/realistic_data.csv`
   - Check "Use Trained Model"
   - Model Path: `models/xgboost_5m.joblib`

---

## ‚úÖ B∆Ø·ªöC 4: Test v√† Verify

### Test Checklist

```bash
# 1. Test simulation mode
streamlit run app.py
# ‚Üí Verify: Dashboard ch·∫°y v·ªõi d·ªØ li·ªáu gi·∫£ l·∫≠p

# 2. Test v·ªõi real data
# Trong sidebar: Production mode + Upload CSV
# ‚Üí Verify: Data info hi·ªÉn th·ªã ƒë√∫ng

# 3. Test v·ªõi model
# Trong sidebar: Check "Use Trained Model"
# ‚Üí Verify: Model info hi·ªÉn th·ªã, predictions kh√°c simulation

# 4. Test metrics
# ‚Üí Verify: MAE, RMSE, MAPE hi·ªÉn th·ªã trong tab "Model Performance"

# 5. Test scaling decisions
# ‚Üí Verify: Scaling events log c√≥ l√Ω do r√µ r√†ng
```

---

## üé¨ B∆Ø·ªöC 5: Chu·∫©n b·ªã Presentation/Demo

### A. Ch·ª•p Screenshots

```python
# capture_screenshots.py
import time
from selenium import webdriver

# M·ªü dashboard
driver = webdriver.Chrome()
driver.get('http://localhost:8502')
time.sleep(5)

# Ch·ª•p full page
driver.save_screenshot('screenshots/dashboard_overview.png')

# Scroll v√† ch·ª•p t·ª´ng ph·∫ßn
# ... (code selenium)
```

### B. Record Video Demo

**Option 1**: OBS Studio
- Download OBS Studio
- Record m√†n h√¨nh dashboard
- Highlight c√°c features

**Option 2**: Streamlit built-in recording
- Dashboard t·ª± ƒë·ªông record th√†nh WebP video trong artifacts

### C. Chu·∫©n b·ªã Slide Presentation

**Outline ƒë·ªÅ xu·∫•t**:

1. **Problem Statement** (1 slide)
   - V·∫•n ƒë·ªÅ: Autoscaling th·ªß c√¥ng kh√¥ng hi·ªáu qu·∫£
   - Solution: AI-powered proactive autoscaling

2. **Data** (1 slide)
   - Ngu·ªìn d·ªØ li·ªáu: [M√¥ t·∫£ ngu·ªìn]
   - Features: timestamp, requests_per_minute
   - Preprocessing: Time features, lag features

3. **Model** (2 slides)
   - Architecture: XGBoost multi-horizon forecasting
   - Training: 80/20 split, time-series aware
   - Performance: MAE, RMSE, MAPE cho 1m/5m/15m

4. **Dashboard** (2 slides)
   - Screenshot: KPI metrics
   - Screenshot: Traffic monitoring chart
   - Screenshot: Model performance tab

5. **Results** (1 slide)
   - Cost savings: X% vs fixed infrastructure
   - Accuracy: MAPE < 15%
   - Response time: Proactive scaling

6. **Demo** (Live)
   - Show dashboard running
   - Upload real data
   - Show predictions
   - Show scaling decisions

---

## üìù Quick Reference Commands

```bash
# Activate environment
.venv\Scripts\Activate.ps1

# Generate realistic data
python generate_realistic_data.py

# Train model
python train_model.py

# Run dashboard
streamlit run app.py

# Access dashboard
# ‚Üí http://localhost:8502
```

---

## üéØ Final Checklist

- [ ] D·ªØ li·ªáu th·∫≠t ƒë√£ chu·∫©n b·ªã (CSV format ƒë√∫ng)
- [ ] Model ƒë√£ train xong (3 models: 1m, 5m, 15m)
- [ ] Dashboard ch·∫°y ƒë∆∞·ª£c v·ªõi production mode
- [ ] Screenshots ƒë√£ ch·ª•p
- [ ] Video demo ƒë√£ record (optional)
- [ ] Slide presentation ƒë√£ chu·∫©n b·ªã
- [ ] ƒê√£ test t·∫•t c·∫£ features

---

**üéâ Ch√∫c b·∫°n demo th√†nh c√¥ng!**
