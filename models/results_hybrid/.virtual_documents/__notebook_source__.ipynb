


# ===========================
# CELL 2: SETUP & INSTALLATIONS
# ===========================

get_ipython().getoutput("pip install prophet tensorflow scikit-learn -q")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.layers import Attention, Input, Concatenate
import warnings
warnings.filterwarnings('ignore')
import os
import pickle
from datetime import datetime
import time
from typing import Dict, List, Tuple

# Prophet
from prophet import Prophet

# TensorFlow/Keras
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Set seeds
np.random.seed(42)
tf.random.set_seed(42)

# Visualization
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (18, 6)

print("="*70)
print("HYBRID PROPHET + LSTM AUTOMATED TRAINING PIPELINE")
print("="*70)
print(f"  TensorFlow version: {tf.__version__}")
print(f"  GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}")
print(f"  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*70)


# ===========================
# CELL 3: GLOBAL CONFIGURATION
# ===========================

# Paths
DATA_DIR = '/kaggle/input/preprocess-data'
RESULTS_BASE_DIR = '/kaggle/working/'

# Create base results directory
os.makedirs(RESULTS_BASE_DIR, exist_ok=True)

# All configurations to run
RESOLUTIONS = ['5min', '15min']
TARGETS = ['request_count', 'total_bytes']

# Resolution-specific parameters
RESOLUTION_PARAMS = {
    '5min': {
        'window': 10,        
        'lstm_units': 50,
        'epochs': 50,
        'batch_size': 50
    },
    '15min': {
        'window': 5,          
        'lstm_units': 50,
        'epochs': 50,
        'batch_size': 50 
    }
}

# Prophet parameters
PROPHET_PARAMS = {
    'daily_seasonality': False,
    'weekly_seasonality': False,
    'yearly_seasonality': False,
    'changepoint_prior_scale': 5,
    'seasonality_prior_scale': 30,
    'seasonality_mode': 'multiplicative'
}

# Storm/outage holiday
STORM_HOLIDAY = pd.DataFrame({
    'holiday': 'storm_outage',
    'ds': pd.date_range(start='1995-08-01 14:52:01', end='1995-08-03 04:36:13', freq='h'),
    'lower_window': 0,
    'upper_window': 0,
})

print("\nðŸ“‹ CONFIGURATION LOADED:")
print(f"  Resolutions: {RESOLUTIONS}")
print(f"  Targets: {TARGETS}")
print(f"  Total configurations: {len(RESOLUTIONS) * len(TARGETS)}")
print(f"\n  Data directory: {DATA_DIR}")
print(f"  Results directory: {RESULTS_BASE_DIR}")


# ===========================
# CELL 4: UTILITY FUNCTIONS
# ===========================

def prepare_prophet_data(df, target_col):
    """
    Prepare data for Prophet.
    """
    prophet_df = pd.DataFrame({
        'ds': df.index,
        'y': df[target_col]
    })
    
    # Add time-based features
    prophet_df['hour'] = prophet_df['ds'].dt.hour
    prophet_df['day_of_week'] = prophet_df['ds'].dt.dayofweek
    prophet_df['is_weekend'] = (prophet_df['day_of_week'] >= 5).astype(int)
    
    return prophet_df


def make_sequences(data, window):
    """
    Create sequences for LSTM.
    """
    X, y = [], []
    for i in range(window, len(data)):
        X.append(data[i-window:i])
        y.append(data[i])
    return np.array(X), np.array(y)

def create_multivariate_features(df, train_size, window):
    """
    Táº¡o bá»™ dá»¯ liá»‡u Ä‘a chiá»u cho LSTM:
    1. Residual (Scaled)
    2. Prophet Prediction (Scaled) - Äá»ƒ há»c Ä‘á»™ lá»›n
    3. Hour Sin/Cos - Äá»ƒ há»c tÃ­nh chu ká»³ thá»i gian
    """
    # 1. Feature: Residual
    resid_values = df[['residual']].values
    scaler_resid = StandardScaler()
    # Chá»‰ fit trÃªn táº­p train
    scaler_resid.fit(resid_values[:train_size]) 
    resid_scaled = scaler_resid.transform(resid_values)
    
    # 2. Feature: Context (Prophet Prediction)
    yhat_values = df[['yhat_prophet']].values
    scaler_context = StandardScaler()
    scaler_context.fit(yhat_values[:train_size])
    yhat_scaled = scaler_context.transform(yhat_values)
    
    # 3. Feature: Time Cyclical Encoding
    # Biáº¿n Ä‘á»•i giá» thÃ nh vÃ²ng trÃ²n (23h gáº§n 0h)
    hour_sin = np.sin(2 * np.pi * df.index.hour / 24).values.reshape(-1, 1)
    hour_cos = np.cos(2 * np.pi * df.index.hour / 24).values.reshape(-1, 1)
    
    # Gá»™p táº¥t cáº£ features: Shape (N, 4)
    # [Residual, Prophet_Hat, Hour_Sin, Hour_Cos]
    features_matrix = np.hstack([resid_scaled, yhat_scaled, hour_sin, hour_cos])
    
    return features_matrix, scaler_resid

def make_multivariate_sequences(features, targets, window):
    """
    X: Multivariate features (t-window ... t-1)
    y: Target residual (t)
    """
    X, y = [], []
    for i in range(window, len(features)):
        X.append(features[i-window:i, :]) # Láº¥y cá»­a sá»• cá»§a Táº¤T Cáº¢ features
        y.append(targets[i])             # Chá»‰ dá»± Ä‘oÃ¡n residual (Ä‘Ã£ scale)
    return np.array(X), np.array(y)


def calculate_metrics(y_true, y_pred, model_name="Model"):
    """
    Calculate comprehensive metrics.
    """
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    
    # MAPE (avoid division by zero)
    mask = y_true != 0
    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.sum() > 0 else 0
    
    # RÂ²
    r2 = 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - y_true.mean())**2))
    
    metrics = {
        'Model': model_name,
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'MAPE': mape,
        'R2': r2
    }
    
    return metrics


def detect_anomalies(y_true, y_pred, window_size, threshold=3):
    """
    Detect anomalies using dynamic Z-score on prediction error.
    """
    error = np.abs(y_true - y_pred)
    
    # Rolling statistics
    rolling_mean = pd.Series(error).rolling(window_size, min_periods=1).mean()
    rolling_std = pd.Series(error).rolling(window_size, min_periods=1).std()
    
    # Z-score
    z_score = (error - rolling_mean) / (rolling_std + 1e-8)
    
    # Anomaly indices
    anomaly_mask = z_score > threshold
    anomaly_indices = np.where(anomaly_mask)[0]
    
    return anomaly_indices, z_score.values


print("âœ“ Utility functions defined")


# # ===========================
# # CELL 5: MAIN HYBRID TRAINING FUNCTION
# # ===========================

# def train_hybrid_model(resolution, target, verbose=True):
#     """
#     Train Hybrid Prophet + LSTM model for a single configuration.
#     """
#     if verbose:
#         print(f"\n{'='*70}")
#         print(f"TRAINING: Hybrid Prophet+LSTM | {resolution} | {target}")
#         print(f"{'='*70}")
    
#     start_time = time.time()
    
#     # Get parameters
#     params = RESOLUTION_PARAMS[resolution]
#     window = params['window']
#     lstm_units = params['lstm_units']
#     epochs = params['epochs']
#     batch_size = params['batch_size']
    
#     # Create results directory
#     results_dir = f"{RESULTS_BASE_DIR}/{resolution}_{target}"
#     os.makedirs(results_dir, exist_ok=True)
#     os.makedirs(f"{results_dir}/prophet_model", exist_ok=True)
    
#     try:
#         # ==================
#         # 1. LOAD DATA
#         # ==================
#         if verbose:
#             print(f"\n[1/7] Loading data...")
        
#         train_df = pd.read_csv(f"{DATA_DIR}/train_{resolution}.csv", index_col=0, parse_dates=True)
#         test_df = pd.read_csv(f"{DATA_DIR}/test_{resolution}.csv", index_col=0, parse_dates=True)
        
#         # Combine for full timeline
#         full_df = pd.concat([train_df, test_df])
#         full_df = full_df.sort_index()
        
#         train_size = len(train_df)
        
#         if verbose:
#             print(f"    Train: {train_size:,} samples")
#             print(f"    Test: {len(test_df):,} samples")
        
#         # ==================
#         # 2. TRAIN PROPHET
#         # ==================
#         if verbose:
#             print(f"\n[2/7] Training Prophet (trend + seasonality)...")
        
#         prophet_train = prepare_prophet_data(train_df, target)
        
#         prophet_model = Prophet(
#             daily_seasonality=PROPHET_PARAMS['daily_seasonality'],
#             weekly_seasonality=PROPHET_PARAMS['weekly_seasonality'],
#             yearly_seasonality=PROPHET_PARAMS['yearly_seasonality'],
#             changepoint_prior_scale=PROPHET_PARAMS['changepoint_prior_scale'],
#             seasonality_prior_scale=PROPHET_PARAMS['seasonality_prior_scale'],
#             seasonality_mode=PROPHET_PARAMS['seasonality_mode'],
#             holidays=STORM_HOLIDAY
#         )
        
#         if resolution == '5min':
#             prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)
#             prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)
#         else:  # 15min
#             prophet_model.add_seasonality(name='daily_pattern', period=1, fourier_order=25)
        
#         prophet_model.add_regressor('hour')
#         prophet_model.add_regressor('day_of_week')
#         prophet_model.add_regressor('is_weekend')
        
#         prophet_model.fit(prophet_train)
        
#         # Forecast full timeline
#         prophet_full = prepare_prophet_data(full_df, target)
#         prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])
        
#         if verbose:
#             print(f"    Prophet forecasting complete")
        
#         # ==================
#         # 3. COMPUTE RESIDUALS
#         # ==================
#         if verbose:
#             print(f"\n[3/7] Computing residuals...")
        
#         full_df['yhat_prophet'] = prophet_forecast['yhat'].values
#         full_df['residual'] = full_df[target] - full_df['yhat_prophet']
        
#         if verbose:
#             print(f"    Residual range: {full_df['residual'].min():.2f} to {full_df['residual'].max():.2f}")
#             print(f"    Residual std: {full_df['residual'].std():.2f}")
        
#         # ==================
#         # 4. PREPARE LSTM DATA
#         # ==================
#         if verbose:
#             print(f"\n[4/7] Preparing LSTM sequences...")
        
#         # Scale residuals
#         scaler = MinMaxScaler(feature_range=(0, 1))
#         residual_train = scaler.fit_transform(
#             full_df.iloc[:train_size][['residual']]
#         )
        
#         # Create sequences
#         X_train, y_train = make_sequences(residual_train.flatten(), window)
#         X_train = X_train.reshape(-1, window, 1)
        
#         if verbose:
#             print(f"    LSTM sequences: {X_train.shape}")
        
#         # ==================
#         # 5. TRAIN LSTM
#         # ==================
#         if verbose:
#             print(f"\n[5/7] Training LSTM (residual patterns)...")
        
#         lstm_model = Sequential([
#             LSTM(lstm_units, input_shape=(window, 1)),
#             Dropout(0.2),
#             Dense(1)
#         ])
        
#         lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        
#         callbacks = [
#             EarlyStopping(patience=5, restore_best_weights=True, verbose=0),
#             ReduceLROnPlateau(patience=5, factor=0.5, verbose=0)
#         ]
        
#         history = lstm_model.fit(
#             X_train, y_train,
#             epochs=epochs,
#             batch_size=batch_size,
#             callbacks=callbacks,
#             verbose=1
#         )
        
#         if verbose:
#             print(f"    LSTM training complete ({len(history.history['loss'])} epochs)")
        
#         # ==================
#         # 6. HYBRID PREDICTION
#         # ==================
#         if verbose:
#             print(f"\n[6/7] Generating hybrid predictions...")
        
#         # Scale all residuals
#         residual_all = scaler.transform(full_df[['residual']])
#         X_all, _ = make_sequences(residual_all.flatten(), window)
#         X_all = X_all.reshape(-1, window, 1)
        
#         # Predict residuals
#         residual_pred = lstm_model.predict(X_all, verbose=0).flatten()
#         residual_pred = scaler.inverse_transform(
#             residual_pred.reshape(-1, 1)
#         ).flatten()
        
#         # Hybrid forecast = Prophet + LSTM residual correction
#         prophet_pred = full_df['yhat_prophet'].iloc[window:].values
#         hybrid_pred = prophet_pred + residual_pred
        
#         # Actual values
#         y_true = full_df[target].iloc[window:].values
        
#         # Split into train and test
#         test_start_idx = train_size - window
        
#         y_test_true = y_true[test_start_idx:]
#         prophet_test_pred = prophet_pred[test_start_idx:]
#         hybrid_test_pred = hybrid_pred[test_start_idx:]
        
#         # LSTM standalone (for comparison)
#         lstm_standalone_pred = full_df[target].iloc[window:].mean() + residual_pred
#         lstm_test_pred = lstm_standalone_pred[test_start_idx:]
        
#         if verbose:
#             print(f"    Test predictions: {len(y_test_true):,} samples")
        
#         # ==================
#         # 7. EVALUATION & ANOMALY DETECTION
#         # ==================
#         if verbose:
#             print(f"\n[7/7] Evaluating models...")
        
#         # Calculate metrics for all three models
#         prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, "Prophet")
#         lstm_metrics = calculate_metrics(y_test_true, lstm_test_pred, "LSTM")
#         hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, "Hybrid")
        
#         if verbose:
#             print(
#                 f"    Prophet - MAE: {prophet_metrics['MAE']:.2f}, "
#                 f"MSE: {prophet_metrics['MSE']:.2f}, "
#                 f"MAPE: {prophet_metrics['MAPE']:.2f}%, "
#                 f"RMSE: {prophet_metrics['RMSE']:.2f}"
#             )
        
#             print(
#                 f"    LSTM Residuals - MAE: {lstm_metrics['MAE']:.2f}, "
#                 f"MSE: {lstm_metrics['MSE']:.2f}, "
#                 f"MAPE: {lstm_metrics['MAPE']:.2f}%, "
#                 f"RMSE: {lstm_metrics['RMSE']:.2f}"
#             )
        
#             print(
#                 f"    Hybrid - MAE: {hybrid_metrics['MAE']:.2f}, "
#                 f"MSE: {hybrid_metrics['MSE']:.2f}, "
#                 f"MAPE: {hybrid_metrics['MAPE']:.2f}%, "
#                 f"RMSE: {hybrid_metrics['RMSE']:.2f}"
#             )

        
#         # ==================
#         # SAVE RESULTS
#         # ==================
        
#         # Save Prophet forecast
#         prophet_forecast.to_csv(f"{results_dir}/prophet_model/forecast.csv", index=False)
        
#         # Save LSTM model
#         lstm_model.save(f"{results_dir}/lstm_model.keras")
        
#         # Save scaler
#         with open(f"{results_dir}/scaler.pkl", 'wb') as f:
#             pickle.dump(scaler, f)
        
#         # Save hybrid predictions
#         test_timestamps = full_df.index[window + test_start_idx:]
#         predictions_df = pd.DataFrame({
#             'timestamp': test_timestamps,
#             'actual': y_test_true,
#             'prophet_pred': prophet_test_pred,
#             'lstm_pred': lstm_test_pred,
#             'hybrid_pred': hybrid_test_pred,
#             'hybrid_residual': y_test_true - hybrid_test_pred
#         })
#         predictions_df.to_csv(f"{results_dir}/hybrid_predictions.csv", index=False)
        
#         # Save anomalies
#         # if len(anomaly_indices) > 0:
#         #     anomalies_df = pd.DataFrame({
#         #         'timestamp': test_timestamps[anomaly_indices],
#         #         'actual': y_test_true[anomaly_indices],
#         #         'predicted': hybrid_test_pred[anomaly_indices],
#         #         'error': np.abs(y_test_true[anomaly_indices] - hybrid_test_pred[anomaly_indices]),
#         #         'z_score': z_scores[anomaly_indices]
#         #     })
#         #     anomalies_df.to_csv(f"{results_dir}/anomalies.csv", index=False)
        
#         # Save metrics comparison
#         metrics_comparison = pd.DataFrame([prophet_metrics, lstm_metrics, hybrid_metrics])
#         metrics_comparison.to_csv(f"{results_dir}/metrics_comparison.csv", index=False)
        
#         # Save configuration
#         config = {
#             'resolution': resolution,
#             'target': target,
#             'window': window,
#             'lstm_units': lstm_units,
#             'epochs_trained': len(history.history['loss']),
#             # 'anomalies_detected': len(anomaly_indices),
#             # 'anomaly_rate': anomaly_rate
#         }
#         config_df = pd.DataFrame([config])
#         config_df.to_csv(f"{results_dir}/configuration.csv", index=False)
        
#         elapsed_time = time.time() - start_time
        
#         if verbose:
#             print(f"\nâœ“ Completed in {elapsed_time:.1f} seconds")
#             print(f"  Results saved to: {results_dir}")
        
#         # Return results for benchmark
#         return {
#             'resolution': resolution,
#             'target': target,
#             'prophet_mae': prophet_metrics['MAE'],
#             'prophet_rmse': prophet_metrics['RMSE'],
#             'prophet_mse': prophet_metrics['MSE'],
#             'prophet_mape': prophet_metrics['MAPE'],
#             'lstm_mae': lstm_metrics['MAE'],
#             'lstm_rmse': lstm_metrics['RMSE'],
#             'lstm_mse': lstm_metrics['MSE'],
#             'lstm_mape': lstm_metrics['MAPE'],
#             'hybrid_mae': hybrid_metrics['MAE'],
#             'hybrid_rmse': hybrid_metrics['RMSE'],
#             'hybrid_mse': hybrid_metrics['MSE'],
#             'hybrid_mape': hybrid_metrics['MAPE'],
#             'hybrid_r2': hybrid_metrics['R2'],
#             # 'anomalies_detected': len(anomaly_indices),
#             # 'anomaly_rate': anomaly_rate,
#             'training_time_sec': elapsed_time,
#             'results_dir': results_dir
#         }
        
#     except Exception as e:
#         print(f"\nâŒ ERROR: {str(e)}")
#         import traceback
#         traceback.print_exc()
#         return None


# print("âœ“ Hybrid training function defined")


# # ===========================
# # CELL 5: MAIN HYBRID TRAINING FUNCTION (UPGRADED LSTM ARCHITECTURE)
# # ===========================

# def train_hybrid_model(resolution, target, verbose=True):
#     """
#     Train Hybrid Prophet + LSTM model for a single configuration.
#     """
#     if verbose:
#         print(f"\n{'='*70}")
#         print(f"TRAINING: Hybrid Prophet+LSTM | {resolution} | {target}")
#         print(f"{'='*70}")
    
#     start_time = time.time()
    
#     # Get parameters
#     params = RESOLUTION_PARAMS[resolution]
#     window = params['window']
#     lstm_units = params['lstm_units']
#     epochs = params['epochs']
#     batch_size = params['batch_size']
    
#     # Create results directory
#     results_dir = f"{RESULTS_BASE_DIR}/{resolution}_{target}"
#     os.makedirs(results_dir, exist_ok=True)
#     os.makedirs(f"{results_dir}/prophet_model", exist_ok=True)
    
#     try:
#         # ==================
#         # 1. LOAD DATA
#         # ==================
#         if verbose:
#             print(f"\n[1/7] Loading data...")
        
#         train_df = pd.read_csv(f"{DATA_DIR}/train_{resolution}.csv", index_col=0, parse_dates=True)
#         test_df = pd.read_csv(f"{DATA_DIR}/test_{resolution}.csv", index_col=0, parse_dates=True)
        
#         # Combine for full timeline
#         full_df = pd.concat([train_df, test_df])
#         full_df = full_df.sort_index()
        
#         train_size = len(train_df)
        
#         if verbose:
#             print(f"    Train: {train_size:,} samples")
#             print(f"    Test: {len(test_df):,} samples")
        
#         # ==================
#         # 2. TRAIN PROPHET
#         # ==================
#         if verbose:
#             print(f"\n[2/7] Training Prophet (trend + seasonality)...")
        
#         prophet_train = prepare_prophet_data(train_df, target)
        
#         prophet_model = Prophet(
#             daily_seasonality=PROPHET_PARAMS['daily_seasonality'],
#             weekly_seasonality=PROPHET_PARAMS['weekly_seasonality'],
#             yearly_seasonality=PROPHET_PARAMS['yearly_seasonality'],
#             changepoint_prior_scale=PROPHET_PARAMS['changepoint_prior_scale'],
#             seasonality_prior_scale=PROPHET_PARAMS['seasonality_prior_scale'],
#             seasonality_mode=PROPHET_PARAMS['seasonality_mode'],
#             holidays=STORM_HOLIDAY
#         )
        
#         if resolution == '5min':
#             prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)
#             prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)
#         else:  # 15min
#             prophet_model.add_seasonality(name='daily_pattern', period=1, fourier_order=25)
        
#         prophet_model.add_regressor('hour')
#         prophet_model.add_regressor('day_of_week')
#         prophet_model.add_regressor('is_weekend')
        
#         prophet_model.fit(prophet_train)
        
#         # Forecast full timeline
#         prophet_full = prepare_prophet_data(full_df, target)
#         prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])
        
#         if verbose:
#             print(f"    Prophet forecasting complete")
        
#         # ==================
#         # 3. COMPUTE RESIDUALS
#         # ==================
#         if verbose:
#             print(f"\n[3/7] Computing residuals...")
        
#         full_df['yhat_prophet'] = prophet_forecast['yhat'].values
#         full_df['residual'] = full_df[target] - full_df['yhat_prophet']
        
#         if verbose:
#             print(f"    Residual range: {full_df['residual'].min():.2f} to {full_df['residual'].max():.2f}")
#             print(f"    Residual std: {full_df['residual'].std():.2f}")
        
#         # ==================
#         # 4. PREPARE LSTM DATA
#         # ==================
#         if verbose:
#             print(f"\n[4/7] Preparing LSTM sequences...")
        
#         # Scale residuals
#         scaler = MinMaxScaler(feature_range=(0, 1))
#         residual_train = scaler.fit_transform(
#             full_df.iloc[:train_size][['residual']]
#         )
        
#         # Create sequences
#         X_train, y_train = make_sequences(residual_train.flatten(), window)
#         X_train = X_train.reshape(-1, window, 1)
        
#         if verbose:
#             print(f"    LSTM sequences: {X_train.shape}")
        
#         # ==================
#         # 5. TRAIN LSTM (UPGRADED ARCHITECTURE)
#         # ==================
#         if verbose:
#             print(f"\n[5/7] Training LSTM (Stacked architecture for residuals)...")
        
#         # --- UPGRADED MODEL DEFINITION ---
#         lstm_model = Sequential([
#             # Layer 1: Return sequences to stack another LSTM layer
#             LSTM(lstm_units, return_sequences=True, input_shape=(window, 1)),
#             Dropout(0.2),
            
#             # Layer 2: Capture deeper temporal dependencies
#             LSTM(lstm_units), 
#             Dropout(0.2),
            
            
#             # Output Layer
#             Dense(1)
#         ])
#         # ---------------------------------
        
#         lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        
#         callbacks = [
#             EarlyStopping(patience=5, restore_best_weights=True, verbose=0),
#             ReduceLROnPlateau(patience=5, factor=0.5, verbose=0)
#         ]
        
#         history = lstm_model.fit(
#             X_train, y_train,
#             epochs=epochs,
#             batch_size=batch_size,
#             callbacks=callbacks,
#             verbose=1
#         )
        
#         if verbose:
#             print(f"    LSTM training complete ({len(history.history['loss'])} epochs)")
        
#         # ==================
#         # 6. HYBRID PREDICTION
#         # ==================
#         if verbose:
#             print(f"\n[6/7] Generating hybrid predictions...")
        
#         # Scale all residuals
#         residual_all = scaler.transform(full_df[['residual']])
#         X_all, _ = make_sequences(residual_all.flatten(), window)
#         X_all = X_all.reshape(-1, window, 1)
        
#         # Predict residuals
#         residual_pred = lstm_model.predict(X_all, verbose=0).flatten()
#         residual_pred = scaler.inverse_transform(
#             residual_pred.reshape(-1, 1)
#         ).flatten()
        
#         # Hybrid forecast = Prophet + LSTM residual correction
#         prophet_pred = full_df['yhat_prophet'].iloc[window:].values
#         hybrid_pred = prophet_pred + residual_pred
        
#         # Actual values
#         y_true = full_df[target].iloc[window:].values
        
#         # Split into train and test
#         test_start_idx = train_size - window
        
#         y_test_true = y_true[test_start_idx:]
#         prophet_test_pred = prophet_pred[test_start_idx:]
#         hybrid_test_pred = hybrid_pred[test_start_idx:]
        
#         # LSTM standalone (for comparison)
#         lstm_standalone_pred = full_df[target].iloc[window:].mean() + residual_pred
#         lstm_test_pred = lstm_standalone_pred[test_start_idx:]
        
#         if verbose:
#             print(f"    Test predictions: {len(y_test_true):,} samples")
        
#         # ==================
#         # 7. EVALUATION & ANOMALY DETECTION
#         # ==================
#         if verbose:
#             print(f"\n[7/7] Evaluating models...")
        
#         # Calculate metrics for all three models
#         prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, "Prophet")
#         lstm_metrics = calculate_metrics(y_test_true, lstm_test_pred, "LSTM")
#         hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, "Hybrid")
        
#         if verbose:
#             print(
#                 f"    Prophet - MAE: {prophet_metrics['MAE']:.2f}, "
#                 f"MSE: {prophet_metrics['MSE']:.2f}, "
#                 f"MAPE: {prophet_metrics['MAPE']:.2f}%, "
#                 f"RMSE: {prophet_metrics['RMSE']:.2f}"
#             )
            
#             print(
#                 f"    LSTM Residuals - MAE: {lstm_metrics['MAE']:.2f}, "
#                 f"MSE: {lstm_metrics['MSE']:.2f}, "
#                 f"MAPE: {lstm_metrics['MAPE']:.2f}%, "
#                 f"RMSE: {lstm_metrics['RMSE']:.2f}"
#             )
            
#             print(
#                 f"    Hybrid - MAE: {hybrid_metrics['MAE']:.2f}, "
#                 f"MSE: {hybrid_metrics['MSE']:.2f}, "
#                 f"MAPE: {hybrid_metrics['MAPE']:.2f}%, "
#                 f"RMSE: {hybrid_metrics['RMSE']:.2f}"
#             )

        
#         # ==================
#         # SAVE RESULTS
#         # ==================
        
#         # Save Prophet forecast
#         prophet_forecast.to_csv(f"{results_dir}/prophet_model/forecast.csv", index=False)
        
#         # Save LSTM model
#         lstm_model.save(f"{results_dir}/lstm_model.keras")
        
#         # Save scaler
#         with open(f"{results_dir}/scaler.pkl", 'wb') as f:
#             pickle.dump(scaler, f)
        
#         # Save hybrid predictions
#         test_timestamps = full_df.index[window + test_start_idx:]
#         predictions_df = pd.DataFrame({
#             'timestamp': test_timestamps,
#             'actual': y_test_true,
#             'prophet_pred': prophet_test_pred,
#             'lstm_pred': lstm_test_pred,
#             'hybrid_pred': hybrid_test_pred,
#             'hybrid_residual': y_test_true - hybrid_test_pred
#         })
#         predictions_df.to_csv(f"{results_dir}/hybrid_predictions.csv", index=False)
        
#         # Save metrics comparison
#         metrics_comparison = pd.DataFrame([prophet_metrics, lstm_metrics, hybrid_metrics])
#         metrics_comparison.to_csv(f"{results_dir}/metrics_comparison.csv", index=False)
        
#         # Save configuration
#         config = {
#             'resolution': resolution,
#             'target': target,
#             'window': window,
#             'lstm_units': lstm_units,
#             'epochs_trained': len(history.history['loss']),
#         }
#         config_df = pd.DataFrame([config])
#         config_df.to_csv(f"{results_dir}/configuration.csv", index=False)
        
#         elapsed_time = time.time() - start_time
        
#         if verbose:
#             print(f"\nâœ“ Completed in {elapsed_time:.1f} seconds")
#             print(f"  Results saved to: {results_dir}")
        
#         # Return results for benchmark
#         return {
#             'resolution': resolution,
#             'target': target,
#             'prophet_mae': prophet_metrics['MAE'],
#             'prophet_rmse': prophet_metrics['RMSE'],
#             'prophet_mse': prophet_metrics['MSE'],
#             'prophet_mape': prophet_metrics['MAPE'],
#             'lstm_mae': lstm_metrics['MAE'],
#             'lstm_rmse': lstm_metrics['RMSE'],
#             'lstm_mse': lstm_metrics['MSE'],
#             'lstm_mape': lstm_metrics['MAPE'],
#             'hybrid_mae': hybrid_metrics['MAE'],
#             'hybrid_rmse': hybrid_metrics['RMSE'],
#             'hybrid_mse': hybrid_metrics['MSE'],
#             'hybrid_mape': hybrid_metrics['MAPE'],
#             'hybrid_r2': hybrid_metrics['R2'],
#             'training_time_sec': elapsed_time,
#             'results_dir': results_dir
#         }
        
#     except Exception as e:
#         print(f"\nâŒ ERROR: {str(e)}")
#         import traceback
#         traceback.print_exc()
#         return None

# print("âœ“ Hybrid training function updated with Stacked LSTM")


# ===========================
# CELL 5: MAIN HYBRID TRAINING FUNCTION (UPGRADED LSTM ARCHITECTURE V2 - ROBUST RESIDUALS)
# ===========================
from sklearn.preprocessing import StandardScaler
from keras.layers import Bidirectional, BatchNormalization
from keras.losses import Huber
import numpy as np

def train_hybrid_model(resolution, target, verbose=True):
    """
    Train Hybrid Prophet + LSTM model for a single configuration.
    Upgraded for better residual forecasting (lower MAPE).
    """
    if verbose:
        print(f"\n{'='*70}")
        print(f"TRAINING: Hybrid Prophet+LSTM | {resolution} | {target}")
        print(f"{'='*70}")
   
    start_time = time.time()
   
    # Get parameters
    params = RESOLUTION_PARAMS[resolution]
    window = params['window']
    lstm_units = params['lstm_units']
    epochs = params['epochs']
    batch_size = params['batch_size']
   
    # Create results directory
    results_dir = f"{RESULTS_BASE_DIR}/{resolution}_{target}"
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(f"{results_dir}/prophet_model", exist_ok=True)
   
    try:
        # ==================
        # 1. LOAD DATA
        # ==================
        if verbose:
            print(f"\n[1/7] Loading data...")
       
        train_df = pd.read_csv(f"{DATA_DIR}/train_{resolution}.csv", index_col=0, parse_dates=True)
        test_df = pd.read_csv(f"{DATA_DIR}/test_{resolution}.csv", index_col=0, parse_dates=True)
       
        # Combine for full timeline
        full_df = pd.concat([train_df, test_df])
        full_df = full_df.sort_index()
       
        train_size = len(train_df)
       
        if verbose:
            print(f" Train: {train_size:,} samples")
            print(f" Test: {len(test_df):,} samples")
       
        # ==================
        # 2. TRAIN PROPHET
        # ==================
        if verbose:
            print(f"\n[2/7] Training Prophet (trend + seasonality)...")
       
        prophet_train = prepare_prophet_data(train_df, target)
       
        prophet_model = Prophet(
            daily_seasonality=PROPHET_PARAMS['daily_seasonality'],
            weekly_seasonality=PROPHET_PARAMS['weekly_seasonality'],
            yearly_seasonality=PROPHET_PARAMS['yearly_seasonality'],
            changepoint_prior_scale=PROPHET_PARAMS['changepoint_prior_scale'],
            seasonality_prior_scale=PROPHET_PARAMS['seasonality_prior_scale'],
            seasonality_mode=PROPHET_PARAMS['seasonality_mode'],
            holidays=STORM_HOLIDAY
        )
       
        if resolution == '5min':
            prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)
            prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)
        else: # 15min hoáº·c 1min
            prophet_model.add_seasonality(name='daily_pattern', period=1, fourier_order=25)
       
        prophet_model.add_regressor('hour')
        prophet_model.add_regressor('day_of_week')
        prophet_model.add_regressor('is_weekend')
       
        prophet_model.fit(prophet_train)
       
        # Forecast full timeline
        prophet_full = prepare_prophet_data(full_df, target)
        prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])
       
        if verbose:
            print(f" Prophet forecasting complete")
       
        # ==================
        # 3. COMPUTE RESIDUALS
        # ==================
        if verbose:
            print(f"\n[3/7] Computing residuals...")
       
        full_df['yhat_prophet'] = prophet_forecast['yhat'].values
        full_df['residual'] = full_df[target] - full_df['yhat_prophet']
       
        if verbose:
            print(f" Residual range: {full_df['residual'].min():.2f} to {full_df['residual'].max():.2f}")
            print(f" Residual std: {full_df['residual'].std():.2f}")
       
        # ==================
        # 4. PREPARE LSTM DATA
        # ==================
        if verbose:
            print(f"\n[4/7] Preparing LSTM sequences...")
       
        # Scale residuals (fit chá»‰ trÃªn train Ä‘á»ƒ trÃ¡nh leakage)
        scaler = StandardScaler()
        residual_train_values = full_df.iloc[:train_size][['residual']].values
        scaler.fit(residual_train_values)
        
        residual_train_scaled = scaler.transform(residual_train_values)
        
        # Create sequences cho train
        X_train, y_train = make_sequences(residual_train_scaled.flatten(), window)
        X_train = X_train.reshape(-1, window, 1)
        
        # Validation split (20% cuá»‘i cá»§a train)
        val_ratio = 0.2
        val_size = int(len(X_train) * val_ratio)
        X_train_sub = X_train[:-val_size]
        y_train_sub = y_train[:-val_size]
        X_val = X_train[-val_size:]
        y_val = y_train[-val_size:]
       
        if verbose:
            print(f" LSTM sequences: {X_train.shape} (train: {len(X_train_sub)}, val: {len(X_val)})")
       
        # ==================
        # 5. TRAIN LSTM (UPGRADED ROBUST ARCHITECTURE)
        # ==================
        if verbose:
            print(f"\n[5/7] Training LSTM (Bidirectional + Huber loss for residuals)...")
       
        # --- UPGRADED ROBUST MODEL ---
        lstm_model = Sequential([
            Bidirectional(LSTM(lstm_units, return_sequences=True, input_shape=(window, 1))),
            Bidirectional(LSTM(lstm_units)),
            Dropout(0.2),
            
            Dense(1)
        ])
        # ---------------------------------
       
        lstm_model.compile(optimizer='adam', loss=Huber(delta=1.0), metrics=['mae'])
       
        callbacks = [
            EarlyStopping(patience=15, restore_best_weights=True, monitor='val_loss', verbose=1),
            ReduceLROnPlateau(patience=7, factor=0.5, monitor='val_loss', verbose=1)
        ]
       
        history = lstm_model.fit(
            X_train_sub, y_train_sub,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
       
        if verbose:
            print(f" LSTM training complete ({len(history.history['loss'])} epochs)")
       
        # ==================
        # 6. HYBRID PREDICTION
        # ==================
        if verbose:
            print(f"\n[6/7] Generating hybrid predictions...")
       
        # Scale all residuals dÃ¹ng scaler Ä‘Ã£ fit trÃªn train
        residual_all_values = full_df[['residual']].values
        residual_all_scaled = scaler.transform(residual_all_values)
        
        X_all, _ = make_sequences(residual_all_scaled.flatten(), window)
        X_all = X_all.reshape(-1, window, 1)
       
        # Predict residuals
        residual_pred_scaled = lstm_model.predict(X_all, verbose=0)
        residual_pred = scaler.inverse_transform(residual_pred_scaled).flatten()
       
        # Hybrid forecast = Prophet + LSTM residual correction
        prophet_pred = full_df['yhat_prophet'].iloc[window:].values
        hybrid_pred = prophet_pred + residual_pred
       
        # Actual values
        y_true = full_df[target].iloc[window:].values
       
        # Split into train and test
        test_start_idx = train_size - window
       
        y_test_true = y_true[test_start_idx:]
        prophet_test_pred = prophet_pred[test_start_idx:]
        hybrid_test_pred = hybrid_pred[test_start_idx:]
       
        # LSTM standalone (for comparison)
        lstm_standalone_pred = full_df[target].iloc[window:].mean() + residual_pred
        lstm_test_pred = lstm_standalone_pred[test_start_idx:]
       
        if verbose:
            print(f" Test predictions: {len(y_test_true):,} samples")
       
        # ==================
        # 7. EVALUATION & ANOMALY DETECTION
        # ==================
        if verbose:
            print(f"\n[7/7] Evaluating models...")
       
        # Calculate metrics
        prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, "Prophet")
        lstm_metrics = calculate_metrics(y_test_true, lstm_test_pred, "LSTM")
        hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, "Hybrid")
       
        # ThÃªm in MAPE riÃªng cá»§a residuals Ä‘á»ƒ theo dÃµi
        residual_test_true = full_df['residual'].iloc[train_size:].values[window - (train_size - test_start_idx):]  # align Ä‘Ãºng
        residual_test_pred = residual_pred[test_start_idx:]
        residual_mape = np.mean(np.abs((residual_test_pred - residual_test_true) / (np.abs(residual_test_true) + 1e-8))) * 100
        
        if verbose:
            print(f"\n>>> LSTM Residuals MAPE on test: {residual_mape:.2f}% <<<")
            print(
                f" Prophet - MAE: {prophet_metrics['MAE']:.2f}, "
                f"MSE: {prophet_metrics['MSE']:.2f}, "
                f"MAPE: {prophet_metrics['MAPE']:.2f}%, "
                f"RMSE: {prophet_metrics['RMSE']:.2f}"
            )
            print(
                f" LSTM Residuals - MAE: {lstm_metrics['MAE']:.2f}, "
                f"MSE: {lstm_metrics['MSE']:.2f}, "
                f"MAPE: {lstm_metrics['MAPE']:.2f}%, "
                f"RMSE: {lstm_metrics['RMSE']:.2f}"
            )
            print(
                f" Hybrid - MAE: {hybrid_metrics['MAE']:.2f}, "
                f"MSE: {hybrid_metrics['MSE']:.2f}, "
                f"MAPE: {hybrid_metrics['MAPE']:.2f}%, "
                f"RMSE: {hybrid_metrics['RMSE']:.2f}"
            )
       
        # ==================
        # SAVE RESULTS
        # ==================
        prophet_forecast.to_csv(f"{results_dir}/prophet_model/forecast.csv", index=False)
        lstm_model.save(f"{results_dir}/lstm_model.keras")
        
        with open(f"{results_dir}/scaler.pkl", 'wb') as f:
            pickle.dump(scaler, f)
        
        test_timestamps = full_df.index[window + test_start_idx:]
        predictions_df = pd.DataFrame({
            'timestamp': test_timestamps,
            'actual': y_test_true,
            'prophet_pred': prophet_test_pred,
            'lstm_pred': lstm_test_pred,
            'hybrid_pred': hybrid_test_pred,
            'hybrid_residual': y_test_true - hybrid_test_pred
        })
        predictions_df.to_csv(f"{results_dir}/hybrid_predictions.csv", index=False)
        
        metrics_comparison = pd.DataFrame([prophet_metrics, lstm_metrics, hybrid_metrics])
        metrics_comparison.to_csv(f"{results_dir}/metrics_comparison.csv", index=False)
        
        config = {
            'resolution': resolution,
            'target': target,
            'window': window,
            'lstm_units': lstm_units,
            'epochs_trained': len(history.history['loss']),
            'residual_mape_test': residual_mape
        }
        config_df = pd.DataFrame([config])
        config_df.to_csv(f"{results_dir}/configuration.csv", index=False)
       
        elapsed_time = time.time() - start_time
       
        if verbose:
            print(f"\nâœ“ Completed in {elapsed_time:.1f} seconds")
            print(f" Results saved to: {results_dir}")
       
        return {
            'resolution': resolution,
            'target': target,
            'prophet_mae': prophet_metrics['MAE'],
            'prophet_rmse': prophet_metrics['RMSE'],
            'prophet_mse': prophet_metrics['MSE'],
            'prophet_mape': prophet_metrics['MAPE'],
            'lstm_mae': lstm_metrics['MAE'],
            'lstm_rmse': lstm_metrics['RMSE'],
            'lstm_mse': lstm_metrics['MSE'],
            'lstm_mape': lstm_metrics['MAPE'],
            'hybrid_mae': hybrid_metrics['MAE'],
            'hybrid_rmse': hybrid_metrics['RMSE'],
            'hybrid_mse': hybrid_metrics['MSE'],
            'hybrid_mape': hybrid_metrics['MAPE'],
            'hybrid_r2': hybrid_metrics['R2'],
            'residual_mape_test': residual_mape,
            'training_time_sec': elapsed_time,
            'results_dir': results_dir
        }
       
    except Exception as e:
        print(f"\nâŒ ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

print("âœ“ Hybrid training function updated - Robust residuals version (StandardScaler + Bidirectional + Huber)")


# ===========================
# CELL 6: RUN ALL CONFIGURATIONS
# ===========================

print("\n" + "="*70)
print("STARTING AUTOMATED HYBRID TRAINING PIPELINE")
print("="*70)

all_results = []
total_configs = len(RESOLUTIONS) * len(TARGETS)
current_config = 0

pipeline_start_time = time.time()

for resolution in RESOLUTIONS:
    for target in TARGETS:
        current_config += 1
        
        print(f"\n\n{'#'*70}")
        print(f"CONFIGURATION {current_config}/{total_configs}")
        print(f"{'#'*70}")
        
        result = train_hybrid_model(resolution, target, verbose=True)
        
        if result is not None:
            all_results.append(result)
            print(f"\nâœ… Configuration {current_config}/{total_configs} completed successfully")
        else:
            print(f"\nâŒ Configuration {current_config}/{total_configs} failed")
        
        # Progress update
        elapsed = time.time() - pipeline_start_time
        avg_time = elapsed / current_config
        remaining = (total_configs - current_config) * avg_time
        
        print(f"\nðŸ“Š Progress: {current_config}/{total_configs} ({current_config/total_configs*100:.1f}%)")
        print(f"   Elapsed: {elapsed/60:.1f} min | Est. remaining: {remaining/60:.1f} min")

total_elapsed = time.time() - pipeline_start_time

print("\n" + "="*70)
print("ALL CONFIGURATIONS COMPLETED")
print("="*70)
print(f"  Total time: {total_elapsed/60:.1f} minutes")
print(f"  Successful: {len(all_results)}/{total_configs}")
print(f"  Failed: {total_configs - len(all_results)}")


# ===========================
# CELL 7: CREATE COMPREHENSIVE BENCHMARK
# ===========================

print("\n" + "="*70)
print("GENERATING COMPREHENSIVE BENCHMARK")
print("="*70)

# Create benchmark directory
benchmark_dir = f"{RESULTS_BASE_DIR}/FINAL_BENCHMARK"
os.makedirs(benchmark_dir, exist_ok=True)

# Convert results to DataFrame
benchmark_df = pd.DataFrame(all_results)

# Save comprehensive comparison
benchmark_file = f"{benchmark_dir}/comprehensive_comparison.csv"
benchmark_df.to_csv(benchmark_file, index=False)
print(f"\nâœ“ Benchmark saved: {benchmark_file}")

print("\nðŸ“Š BENCHMARK RESULTS:\n")
display(benchmark_df.style.background_gradient(
    cmap='RdYlGn_r', 
    subset=['prophet_mae', 'lstm_mae', 'hybrid_mae', 'hybrid_rmse']
).format({
    'prophet_mae': '{:.2f}',
    'prophet_rmse': '{:.2f}',
    'prophet_mse': '{:.2f}',
    'prophet_mape': '{:.2f}',
    'lstm_mae': '{:.2f}',
    'lstm_rmse': '{:.2f}',
    'lstm_mse': '{:.2f}',
    'lstm_mape': '{:.2f}',
    'hybrid_mae': '{:.2f}',
    'hybrid_rmse': '{:.2f}',
    'hybrid_mse': '{:.2f}',
    'hybrid_mape': '{:.2f}%',
    'hybrid_r2': '{:.4f}',
    'training_time_sec': '{:.1f}s'
}))

# Calculate improvements
print("\n" + "="*70)
print("HYBRID IMPROVEMENTS")
print("="*70)

for idx, row in benchmark_df.iterrows():
    prophet_imp = ((row['prophet_mae'] - row['hybrid_mae']) / row['prophet_mae']) * 100
    lstm_imp = ((row['lstm_mae'] - row['hybrid_mae']) / row['lstm_mae']) * 100
    
    print(f"\n{row['resolution']} | {row['target']}:")
    print(f"  Hybrid MAE: {row['hybrid_mae']:.2f}")
    print(f"  Improvement over Prophet: {prophet_imp:+.1f}%")
    print(f"  Improvement over LSTM: {lstm_imp:+.1f}%")
    # print(f"  Anomaly rate: {row['anomaly_rate']:.2f}%")

# Overall statistics
print("\n" + "="*70)
print("OVERALL STATISTICS")
print("="*70)

avg_prophet_imp = ((benchmark_df['prophet_mae'] - benchmark_df['hybrid_mae']) / benchmark_df['prophet_mae']).mean() * 100
avg_lstm_imp = ((benchmark_df['lstm_mae'] - benchmark_df['hybrid_mae']) / benchmark_df['lstm_mae']).mean() * 100

print(f"\nAverage Hybrid Improvement:")
print(f"  vs Prophet: {avg_prophet_imp:+.1f}%")
print(f"  vs LSTM: {avg_lstm_imp:+.1f}%")
# print(f"\nAverage Anomaly Rate: {benchmark_df['anomaly_rate'].mean():.2f}%")
# print(f"Total Anomalies Detected: {benchmark_df['anomalies_detected'].sum():,}")


# ===========================
# CELL 8: BENCHMARK VISUALIZATIONS
# ===========================

print("\n[CREATING BENCHMARK VISUALIZATIONS]\n")

fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# 1. MAE Comparison: Prophet vs LSTM vs Hybrid
ax1 = fig.add_subplot(gs[0, :])
x_pos = np.arange(len(benchmark_df))
width = 0.25

ax1.bar(x_pos - width, benchmark_df['prophet_mae'], width, label='Prophet', alpha=0.8, color='#2a9d8f')
ax1.bar(x_pos, benchmark_df['lstm_mae'], width, label='LSTM', alpha=0.8, color='#e76f51')
ax1.bar(x_pos + width, benchmark_df['hybrid_mae'], width, label='Hybrid', alpha=0.8, color='#f4a261')

ax1.set_title('MAE Comparison: Prophet vs LSTM vs Hybrid', fontweight='bold', fontsize=14)
ax1.set_ylabel('MAE')
ax1.set_xlabel('Configuration')
ax1.set_xticks(x_pos)
ax1.set_xticklabels([f"{r['resolution']}\n{r['target'][:8]}" for _, r in benchmark_df.iterrows()], 
                     rotation=0, fontsize=9)
ax1.legend()
ax1.grid(True, alpha=0.3, axis='y')

# 2. Hybrid Improvement over Prophet
ax2 = fig.add_subplot(gs[1, 0])
improvements = ((benchmark_df['prophet_mae'] - benchmark_df['hybrid_mae']) / benchmark_df['prophet_mae']) * 100
colors = ['green' if x > 0 else 'red' for x in improvements]
ax2.barh(range(len(improvements)), improvements, color=colors, alpha=0.7)
ax2.set_title('Hybrid Improvement over Prophet', fontweight='bold', fontsize=12)
ax2.set_xlabel('Improvement (%)')
ax2.set_yticks(range(len(benchmark_df)))
ax2.set_yticklabels([f"{r['resolution']} {r['target'][:8]}" for _, r in benchmark_df.iterrows()], 
                     fontsize=9)
ax2.axvline(x=0, color='black', linestyle='--', linewidth=1)
ax2.grid(True, alpha=0.3, axis='x')

# 3. RMSE Comparison
ax3 = fig.add_subplot(gs[1, 1])
x_pos = np.arange(len(benchmark_df))
ax3.plot(x_pos, benchmark_df['prophet_rmse'], 'o-', label='Prophet', linewidth=2, markersize=8)
ax3.plot(x_pos, benchmark_df['lstm_rmse'], 's-', label='LSTM', linewidth=2, markersize=8)
ax3.plot(x_pos, benchmark_df['hybrid_rmse'], '^-', label='Hybrid', linewidth=2, markersize=8)
ax3.set_title('RMSE Comparison', fontweight='bold', fontsize=12)
ax3.set_ylabel('RMSE')
ax3.set_xlabel('Configuration')
ax3.set_xticks(x_pos)
ax3.set_xticklabels([f"{r['resolution']}" for _, r in benchmark_df.iterrows()], fontsize=9)
ax3.legend()
ax3.grid(True, alpha=0.3)

# # 4. Anomaly Detection Rate
# ax4 = fig.add_subplot(gs[1, 2])
# ax4.bar(range(len(benchmark_df)), benchmark_df['anomaly_rate'], color='#e63946', alpha=0.7)
# ax4.set_title('Anomaly Detection Rate', fontweight='bold', fontsize=12)
# ax4.set_ylabel('Anomaly Rate (%)')
# ax4.set_xlabel('Configuration')
# ax4.set_xticks(range(len(benchmark_df)))
# ax4.set_xticklabels([f"{r['resolution']}\n{r['target'][:8]}" for _, r in benchmark_df.iterrows()], 
#                      rotation=0, fontsize=9)
# ax4.grid(True, alpha=0.3, axis='y')

# 5. RÂ² Score
ax5 = fig.add_subplot(gs[2, 0])
ax5.bar(range(len(benchmark_df)), benchmark_df['hybrid_r2'], color='#2a9d8f', alpha=0.7)
ax5.set_title('Hybrid RÂ² Score', fontweight='bold', fontsize=12)
ax5.set_ylabel('RÂ²')
ax5.set_xlabel('Configuration')
ax5.set_xticks(range(len(benchmark_df)))
ax5.set_xticklabels([f"{r['resolution']}" for _, r in benchmark_df.iterrows()], fontsize=9)
ax5.grid(True, alpha=0.3, axis='y')

# 6. Training Time
ax6 = fig.add_subplot(gs[2, 1])
ax6.bar(range(len(benchmark_df)), benchmark_df['training_time_sec'], color='#f4a261', alpha=0.7)
ax6.set_title('Training Time', fontweight='bold', fontsize=12)
ax6.set_ylabel('Time (seconds)')
ax6.set_xlabel('Configuration')
ax6.set_xticks(range(len(benchmark_df)))
ax6.set_xticklabels([f"{r['resolution']}" for _, r in benchmark_df.iterrows()], fontsize=9)
ax6.grid(True, alpha=0.3, axis='y')

# 7. Model Comparison Summary
ax7 = fig.add_subplot(gs[2, 2])
wins = {
    'Prophet': sum(benchmark_df['prophet_mae'] < benchmark_df[['lstm_mae', 'hybrid_mae']].min(axis=1)),
    'LSTM': sum(benchmark_df['lstm_mae'] < benchmark_df[['prophet_mae', 'hybrid_mae']].min(axis=1)),
    'Hybrid': sum(benchmark_df['hybrid_mae'] < benchmark_df[['prophet_mae', 'lstm_mae']].min(axis=1))
}
colors_pie = ['#2a9d8f', '#e76f51', '#f4a261']
ax7.pie(wins.values(), labels=wins.keys(), autopct='%1.0f%%', colors=colors_pie, startangle=90)
ax7.set_title('Best Model by Configuration\n(Lowest MAE)', fontweight='bold', fontsize=12)

plt.suptitle('Hybrid Prophet+LSTM: Comprehensive Benchmark Analysis', 
            fontsize=16, fontweight='bold', y=0.995)

viz_file = f"{benchmark_dir}/benchmark_visualizations.png"
plt.savefig(viz_file, dpi=150, bbox_inches='tight')
plt.show()

print(f"âœ“ Visualizations saved: {viz_file}")


# ===========================
# CELL 9: GENERATE FINAL REPORT
# ===========================

print("\n[GENERATING FINAL REPORT]\n")

# Find best overall configuration
best_idx = benchmark_df['hybrid_mae'].idxmin()
best = benchmark_df.loc[best_idx]

report = f"""
{'='*80}
HYBRID PROPHET + LSTM: COMPREHENSIVE BENCHMARK REPORT
{'='*80}

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Total Pipeline Time: {total_elapsed/60:.1f} minutes

{'='*80}
HYBRID ARCHITECTURE
{'='*80}

The hybrid model combines two complementary approaches:

1. Prophet Component:
   - Captures long-term trends
   - Models daily/weekly seasonality
   - Handles holidays and special events
   - Provides interpretable forecasts

2. LSTM Component:
   - Learns from Prophet's residuals
   - Captures short-term patterns
   - Corrects systematic errors
   - Improves accuracy

3. Final Forecast:
   Hybrid = Prophet_Forecast + LSTM_Residual_Correction

{'='*80}
CONFIGURATIONS TESTED
{'='*80}

Resolutions: {', '.join(RESOLUTIONS)}
Target Variables: {', '.join(TARGETS)}

Total Configurations: {total_configs}
Successful: {len(all_results)}
Failed: {total_configs - len(all_results)}

{'='*80}
OVERALL PERFORMANCE
{'='*80}

Average Metrics (Hybrid):
  MAE:  {benchmark_df['hybrid_mae'].mean():.2f} (Â±{benchmark_df['hybrid_mae'].std():.2f})
  RMSE: {benchmark_df['hybrid_rmse'].mean():.2f} (Â±{benchmark_df['hybrid_rmse'].std():.2f})
  MAPE: {benchmark_df['hybrid_mape'].mean():.2f}% (Â±{benchmark_df['hybrid_mape'].std():.2f}%)
  RÂ²:   {benchmark_df['hybrid_r2'].mean():.4f} (Â±{benchmark_df['hybrid_r2'].std():.4f})

Average Improvement:
  vs Prophet: {avg_prophet_imp:+.1f}%
  vs LSTM:    {avg_lstm_imp:+.1f}%

{'='*80}
BEST OVERALL CONFIGURATION
{'='*80}

Resolution: {best['resolution']}
Target: {best['target']}

Performance:
  Hybrid MAE:  {best['hybrid_mae']:.2f}
  Hybrid RMSE: {best['hybrid_rmse']:.2f}
  Hybrid MAPE: {best['hybrid_mape']:.2f}%
  Hybrid RÂ²:   {best['hybrid_r2']:.4f}

Improvements:
  vs Prophet: {((best['prophet_mae'] - best['hybrid_mae']) / best['prophet_mae'] * 100):+.1f}%
  vs LSTM:    {((best['lstm_mae'] - best['hybrid_mae']) / best['lstm_mae'] * 100):+.1f}%


{'='*80}
PERFORMANCE BY RESOLUTION
{'='*80}

"""

for resolution in RESOLUTIONS:
    subset = benchmark_df[benchmark_df['resolution'] == resolution]
    if len(subset) > 0:
        avg_improvement = ((subset['prophet_mae'] + subset['lstm_mae']) / 2 - subset['hybrid_mae']).mean()
        avg_improvement_pct = (avg_improvement / ((subset['prophet_mae'] + subset['lstm_mae']) / 2).mean()) * 100
        
        report += f"""
{resolution}:
  Hybrid MAE:  {subset['hybrid_mae'].mean():.2f}
  Hybrid RMSE: {subset['hybrid_rmse'].mean():.2f}
  Avg Improvement: {avg_improvement_pct:+.1f}% (vs avg of Prophet & LSTM)
"""

report += f"""
{'='*80}
KEY FINDINGS
{'='*80}

1. Hybrid Advantage:
   - Hybrid outperforms individual models in {sum(benchmark_df['hybrid_mae'] < benchmark_df[['prophet_mae', 'lstm_mae']].min(axis=1))}/{len(benchmark_df)} cases
   - Average improvement: {((benchmark_df['prophet_mae'] + benchmark_df['lstm_mae'])/2 - benchmark_df['hybrid_mae']).mean():.2f} MAE points

2. Best Resolution:
   - {benchmark_df.groupby('resolution')['hybrid_mae'].mean().idxmin()} achieves lowest MAE
   - Average MAE: {benchmark_df.groupby('resolution')['hybrid_mae'].mean().min():.2f}

4. Model Complementarity:
   - Prophet captures seasonal patterns
   - LSTM corrects residual errors
   - Combined approach yields {avg_prophet_imp:+.1f}% improvement

{'='*80}
RECOMMENDATIONS FOR AUTOSCALING
{'='*80}

1. Primary Recommendation:
   - Use {best['resolution']} resolution for {best['target']}
   - Deploy Hybrid model (MAE: {best['hybrid_mae']:.2f})
   - Expected accuracy: {100 - best['hybrid_mape']:.1f}%

2. Scaling Strategy:
   - Use hybrid forecast as baseline
   - Add safety buffer: forecast + 2Ïƒ
   - Ïƒ â‰ˆ {benchmark_df['hybrid_mae'].mean():.0f} (average MAE)
   - Safety margin: ~{2 * benchmark_df['hybrid_mae'].mean():.0f} units

4. Retraining Schedule:
   - Retrain Prophet weekly (trend + seasonality)
   - Retrain LSTM daily (recent patterns)
   - Full retrain if MAE increases >20%
   - Monitor residual patterns continuously

5. Production Deployment:
   - Prophet: Load forecast and parameters
   - LSTM: Load model.keras and scaler.pkl
   - Combine predictions: prophet + lstm_residual
   - Run anomaly detection in parallel

{'='*80}
FILES GENERATED
{'='*80}

Benchmark Files:
  â€¢ comprehensive_comparison.csv - All metrics for all configurations
  â€¢ benchmark_visualizations.png - Visual comparison
  â€¢ final_report.txt - This report

Individual Configuration Results:
  â€¢ prophet_model/forecast.csv - Prophet forecasts
  â€¢ lstm_model.keras - LSTM residual model
  â€¢ scaler.pkl - Residual scaler
  â€¢ hybrid_predictions.csv - All three model predictions
  â€¢ anomalies.csv - Detected anomalies
  â€¢ metrics_comparison.csv - Prophet vs LSTM vs Hybrid
  â€¢ configuration.csv - Model parameters

{'='*80}
END OF REPORT
{'='*80}
"""

# Save report
report_file = f"{benchmark_dir}/final_report.txt"
with open(report_file, 'w') as f:
    f.write(report)

print(report)

print(f"\nâœ“ Final report saved: {report_file}")


# ===========================
# CELL 10: SUMMARY & NEXT STEPS
# ===========================

print("\n" + "="*80)
print("ðŸŽ‰ AUTOMATED HYBRID PIPELINE COMPLETED SUCCESSFULLY!")
print("="*80)

print(f"\nðŸ“Š SUMMARY:")
print(f"  Total configurations: {len(all_results)}")
print(f"  Total time: {total_elapsed/60:.1f} minutes")
print(f"  Average time per config: {total_elapsed/len(all_results):.1f} seconds")

print(f"\nðŸ“ RESULTS LOCATION:")
print(f"  Main directory: {RESULTS_BASE_DIR}")
print(f"  Benchmark: {benchmark_dir}")

print(f"\nðŸ† BEST CONFIGURATION:")
print(f"  {best['resolution']} | {best['target']}")
print(f"  Hybrid MAE: {best['hybrid_mae']:.2f}")
print(f"  Improvement: {avg_prophet_imp:+.1f}% vs Prophet, {avg_lstm_imp:+.1f}% vs LSTM")

# print(f"\nðŸ” ANOMALY DETECTION:")
# print(f"  Total anomalies: {benchmark_df['anomalies_detected'].sum():,}")
# print(f"  Average rate: {benchmark_df['anomaly_rate'].mean():.2f}%")

print(f"\nðŸ“ˆ TOP 3 PERFORMERS (by Hybrid MAE):")
top_3 = benchmark_df.nsmallest(3, 'hybrid_mae')[['resolution', 'target', 'hybrid_mae', 'hybrid_rmse']]
for idx, row in top_3.iterrows():
    print(f"  {row['resolution']:5s} | {row['target']:15s} | MAE: {row['hybrid_mae']:6.2f} | RMSE: {row['hybrid_rmse']:6.2f}")

print(f"\nðŸ’¡ NEXT STEPS:")
print(f"  1. Review final_report.txt in {benchmark_dir}")
print(f"  2. Check benchmark_visualizations.png")
print(f"  3. Analyze anomalies.csv for each configuration")
print(f"  4. Deploy best hybrid model to production")
print(f"  5. Set up monitoring for anomaly alerts")
print(f"  6. Implement retraining pipeline (weekly Prophet, daily LSTM)")

print(f"\n" + "="*80)
print("All results have been saved to Google Drive!")
print("="*80)


# NÃ©n toÃ n bá»™ file trong thÆ° má»¥c hiá»‡n táº¡i (recursive)
get_ipython().getoutput("zip -r all_output.zip .")

# Táº¡o link táº£i
from IPython.display import FileLink
FileLink('all_output.zip')
